Discutiremos o layout e o design de nossa negociação com aprendizado por reforço, mas primeiro em um nível muito alto.
Há dois modos de operação: Treine e teste.

Como sempre, queremos que todos os nossos dados de treinamento estejam no passado e todos os dados de teste sejam preços de ações que vieram após os dados de treinamento. Então, vamos treinar nosso agente para maximizar sua recompensa em um episódio usando apenas os dados de treinamento. Em seguida, usaremos esse agente treinado nos dados de teste para ver qual é o valor do nosso portfólio.

A parte principal do código será mais ou menos assim: crie uma instância do ambiente e, em seguida, crie uma instância do agente. Não se preocupe com o que isso ainda faz; então, em um loop, teremos uma função chamada play_one_episode, que aceita o ambiente e o agente e retorna o valor do portfólio no final do episódio.

Quando nosso loop terminar, salvaremos os valores do portfólio para mais tarde, para que possamos plotá-los e analisá-los. Portanto, isso é bastante simples, mas agora temos que descobrir o que deve acontecer na função play_one_episode. Como sempre, começamos a redefinir o ambiente para voltar ao estágio inicial. Em seguida, inicializamos nosso sinalizador e inserimos um loop que fecha somente quando concluído se torna verdadeiro dentro do loop.

Nós escolhemos uma ação. Agora, neste ponto, você sabe que esta ação é proveniente de nosso agente, mas adiará como o agente funciona até mais tarde. Vamos chamar essa visão de função de etapa para executar a ação e receber a recompensa do dia seguinte. E assim, a seguir, vamos verificar se nosso script está no modo de treinamento, se for o caso, precisamos treinar nosso agente. Depois, chamaremos a reprodução do agente para obter uma amostra do nosso buffer de reprodução e executar uma etapa de descida gradiente. Um detalhe adicional a ser lembrado é que nossos dados ainda não estão normalizados.

Você pode imaginar que nosso estado, composto por três partes, pode ter faixas muito diferentes. A primeira parte consiste no número de ações que possuímos. A segunda parte consiste nos preços das ações na terceira parte, em quanto dinheiro investimos. Então, queremos normalizar esses dados. Podemos fazer isso de maneira muito simples sempre que obtivermos um novo estado. Teremos um objeto escalar do aprendizado que tomará nosso estado e o padronizará para ter média zero e variação unitária. Portanto, não é uma grande adição ao nosso código anterior a seguir.

Vamos imaginar como os objetos do nosso ambiente serão realmente. Primeiro, ele aceitará uma série temporal de preços das ações como insumo para o construtor. Também teremos um ponteiro para nos dizer que dia é hoje. Portanto, sabemos os preços atuais das ações e, ao mesmo tempo, queremos saber quanto dinheiro começamos inicialmente com nosso investimento inicial, pois podemos fazer tudo o que for necessário para fazer nossa função de redefinição. Fique claro que deve ser todo em dinheiro e sem investimento, nossa função de etapa executará uma ação e, em seguida, comprará e venderá as ações especificadas pela ação e definirá nosso indicador para os preços das ações do dia seguinte.

Também calcularemos o dia seguinte e o valor do portfólio. A partir disso, podemos calcular a recompensa. O sinalizador concluído será definido como verdadeiro. Se chegarmos ao final de nossa série cronológica, basicamente para o ambiente, temos o Construtor como uma função de redefinição e uma função de etapa. Finalmente, vamos considerar nosso objeto de agente que é complicado, mas não mais complicado que o ambiente, as partes essenciais do agente. O buffer de repetição que é o que usamos para armazenar nossas transições no ambiente e um neurônio que funciona ou algum outro tipo de modelo que é o que usaremos para aproximar nossos valores Q.

Então, quais são as funções essenciais do agente, como vimos. Precisamos de uma função chamada update replay memory que recebe uma recompensa de ação de estado no dia seguinte e o flag concluído e armazena isso em nosso objeto de buffer de repetição. Em seguida, vamos precisar de um get. Função de ação que aceita como entrada um estado e decide qual ação executar no ambiente.

E como esse é o Q-learning, ele usará a regra de Q-learning ou alguma variante dela, como a Epsilon greedy, finalmente precisaremos de uma função de repetição que faça o seguinte. Primeiro, ele pega uma amostra aleatória do buffer de reprodução a seguir.

Ele usa isso para calcular um conjunto de dados de aprendizado supervisionado que consiste em pares de entrada e de destino, que é o que precisamos para treinar nosso modelo depois de termos nosso conjunto de dados. Podemos chamar modelo que treina para executar uma iteração de descida de gradiente.